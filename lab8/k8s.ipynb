{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Serving Models on KuberNetes\n",
    "\n",
    "As you may notice, different service may need different resources (e.g., GPU for LLM service) and dependencies. In this experiement, you are required to deploy your applications on our cluster. You should implement Dockerfiles that build images that serve your applications.\n",
    "\n",
    "Additional requirement: serve multiple LLMs simultaneously and support switching background models at the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a simple example dockerfile for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Dockerfile.fastapi_simple\n",
    "\n",
    "# Use an NVIDIA PyTorch image, here xx.xx should be your docker version\n",
    "# For example, if `docker --version` outputs 20.10.03, xx.xx should be 20.10\n",
    "# FROM nvcr.io/nvidia/pytorch:xx.xx-py3\n",
    "FROM nvcr.io/nvidia/pytorch:20.10-py3\n",
    "\n",
    "# Change the source of pip to Tsinghua Tuna, and install\n",
    "# Hugging Face dependencies. Add your dependencies here.\n",
    "RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple && pip install uvicorn fastapi --use-feature=2020-resolver \n",
    "\n",
    "# Following you should put the logic of running your service\n",
    "WORKDIR /app\n",
    "COPY ./fastapi_service_simple.py /app\n",
    "CMD [\"uvicorn\", \"fastapi_service_simple:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file fastapi_service_simple.py\n",
    "\n",
    "import fastapi\n",
    "\n",
    "\n",
    "app = fastapi.FastAPI()\n",
    "\n",
    "\n",
    "@app.get('/inference')\n",
    "def process_string(data: str):\n",
    "    return f'Processed {data} by FastAPI!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Dockerfile.gradio_simple\n",
    "# Use an official Python runtime as a parent image with Python 3.11\n",
    "FROM python:3.11\n",
    "\n",
    "# Change the source of pip to Tsinghua Tuna, and install\n",
    "# Hugging Face dependencies. Add your dependencies here.\n",
    "RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple && pip install gradio\n",
    "\n",
    "WORKDIR /app\n",
    "COPY ./gradio_service_simple.py /app\n",
    "CMD [\"python\", \"gradio_service_simple.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gradio_service_simple.py\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return requests.get(f'http://fastapi-service:8000/inference?data={name}').text\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name='0.0.0.0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build images using the following commands.\n",
    "```bash\n",
    "# Build the images\n",
    "docker build -t my-fastapi-app -f Dockerfile.fastapi_simple .\n",
    "docker build -t my-gradio-app -f Dockerfile.gradio_simple .\n",
    "```\n",
    "Push the image to the cluster:\n",
    "```bash\n",
    "docker login [hub_addr]\n",
    "docker tag my-fastapi-app [hub_addr]/[your_project]/my-fastapi-app\n",
    "docker tag my-gradio-app [hub_addr]/[your_project]/my-gradio-app\n",
    "docker push [hub_addr]/[your_project]/my-fastapi-app\n",
    "docker push [hub_addr]/[your_project]/my-gradio-app\n",
    "```\n",
    "`[hub_addr]` is the address of the cluster harbor.\n",
    "\n",
    "You may use the following config files to deploy your applications. Another hint: you can expose your service using `kubectl port-forward` like this:\n",
    "\n",
    "```bash\n",
    "kubectl port-forward pod/[my-pod] [local_port]:[pod_port]\n",
    "```\n",
    "\n",
    "The following are some additional examples for using k8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file fastapi-deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: fastapi-deployment\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: fastapi\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: fastapi\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fastapi\n",
    "        image: my-fastapi-app\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file fastapi-service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: fastapi-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: fastapi\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "      targetPort: 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gradio-deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: gradio-deployment\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: gradio\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: gradio\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: gradio\n",
    "        image: my-gradio-app\n",
    "        ports:\n",
    "        - containerPort: 7860\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gradio-service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: gradio-service\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: gradio\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 7860\n",
    "      targetPort: 7860"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

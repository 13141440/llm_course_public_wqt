{"nbformat_minor": 2, "nbformat": 4, "cells": [{"source": ["# Lab 1: Try LLM APIs"], "cell_type": "markdown", "metadata": {}}, {"source": ["## You will learn:\n", "- First experience how to do run program on the cloud\n", "- Learn how to manage API keys\n", "- Frist experience of using different LLM APIs\n", "- (If you haven't used it before), how to use Jupyter Notebook in VSCode"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 0 Preparations"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 0.1 Dependencies"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# requirements.txt contains the basic packages needed to implement this project\n", "# We have installed all dependencies in the default image, so you do not have to install them again, if you use the default image.\n", "#!pip install -r requirements.txt"], "outputs": [], "metadata": {}}, {"source": ["### 0.2 Proxy to access OpenAI"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# add proxy to access openai ...\n", "import os\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""], "outputs": [], "metadata": {}}, {"source": ["### 0.3 Saving your API token in a .env file\n", "\n", "Here we need to use an OpenAI API KEY, and passing the API KEY directly into the code is terrible for security. A more correct way is to use the dotenv package, write the API_KEY to the .env file, and import the API KEY later by loading the environment variable.\n", "\n", "You should first create a new file named .env in the root directory of your project.** (AND Never commit it to Git!)\n", "The content in this file should be stored as key-value pair.  The .env file is simply a text file with one key-value per line like:\n", "\n", "    # Comment 1\n", "    KEY1=value1\n", "    # Comment 2\n", "    KEY2=value2\n", "\n", "More informations see:\n", "\n", "https://pythonjishu.com/ifggzibrpkgavow/ "], "cell_type": "markdown", "metadata": {}}, {"source": ["##  1 Using OpenAI API"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 1.1 Get response from OpenAI"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from dotenv import load_dotenv\n", "import os\n", "load_dotenv()\n", "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n", "# print(openai_api_key)  ## if you print it, make sure that you clear the output before commiting the notebook to github."], "outputs": [], "metadata": {}}, {"source": ["Then you can make a call to OpenAI, using the api-key. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from openai import OpenAI\n", "client = OpenAI(api_key=openai_api_key)\n", "\n", "response = client.chat.completions.create(\n", "  model=\"gpt-4-turbo-preview\",\n", "  messages=[\n", "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n", "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n", "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n", "  ]\n", ")\n", "print(response.choices[0].message.content)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "# You can exlore what information is in the response object by printing it out and examine it\n"], "outputs": [], "metadata": {}}, {"source": ["You can learn more about the OpenAI API from https://platform.openai.com/docs/overview"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1.2 Your Task: Use OpenAI api to achieve one shift Caesar cipher communicate \n", "\n", "We have already provided you the prompts, and you should consider the instruction and demonstrations, and you should use the model gpt-4-turbo-preview."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["def encode(s):\n", "    for c in s:\n", "        if c not in ' ,.!?':\n", "            c = chr(ord(c) + 1)\n", "        print(c, end='')    \n", "        \n", "def decode(s):\n", "    for c in s:\n", "        if c not in ' ,.!?':\n", "            c = chr(ord(c) - 1)\n", "        print(c, end='')"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["encode('What is the capital of France?')"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["prompt = \"\"\"\n", "You are an expert on Caesar Cipher. We will communicate in Caesar. Do not be a translator.\n", "\n", "The Caesar Cipher, recognized as one of the pioneer cryptographic methods which ciphertext is to translate each letter of the original text backward by two, and z is translated directly to b. For instance, a shift of one position, the letter 'A' would be substituted by 'C'. you should answer my question in Caesar.\n", "\n", "Examples:\n", "\n", "User: ipx up nblf b cpnc ?\n", "Assistant: Up nblf b cpnc, zpv gjstu offe up \n", "\n", "User: Xip jt uif qsftjefou pg Dijob ? \n", "Assistant: Wh Ihmfohmf.\n", "\n", "User: Dbo zpv ufmm nf xifsf jt uif dbqjubm pg Dijob ?\n", "Assistant: Cfjkjoh.\n", "\n", "User: Dbo zpv ufmm nf xifsf jt uif dbqjubm pg Bnfsjdbo ?\n", "Assistant: Xbtijohupo.\n", "\n", "User: Xibu jt uif dbqjubm pg Gsbodf ?\"\"\"\n", "\n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "# using the gpt-4 model, create a chat response to the prompt above\n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "### TODO: Print out the cipher text here\n", "### TODO: Print out the clear text here using the decode() function\n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "### TODO: print out the response object.  Explore the entire response object.  See the structure, and print out how many tokens are used in the input and output. \n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "### TODO: Repeat the query with another model 'gpt-3.5-turbo'.  Do you still get the same response?"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "### TODO: (optional) can you let 'gpt-3.5-turbo' to print the same, by adding more examples in the prompt?  \n", "### Consider using a script to generate a much longer prompt with more examples"], "outputs": [], "metadata": {}}, {"source": ["## 2. Play with LLMs deployed on this cluster"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 2.1 Try the local API"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import os\n", "import re\n", "import requests\n", "import json"], "outputs": [], "metadata": {}}, {"source": ["First, you will need to request for an API token from our local service. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["get_res = requests.get('http://10.1.0.5:32411/auth/callback/debug?code=dev')\n", "access_token = eval(get_res.text)['access_token']['access_token']\n", "# print(access_token)  ## again, if you print it you need to clear the token from output before committing the notebook to github!"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# you can choose the model to call. \n", "# The models currently provided include Qwen-7b-chat, vicuna and Llama-2-7B-Chat-fp16\n", "model = 'vicuna'\n", "api_url = 'http://10.1.0.5:32411/v1/chat/completions'"], "outputs": [], "metadata": {}}, {"source": ["The local service provides a raw API, exposing the HTTP request details.  You should construct a HTTP query as the following"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["headers={'Content-Type': 'application/json',\n", "         'Authorization': 'Bearer '+ access_token\n", "         }\n", "\n", "request = {\"model\": model, \n", "           \"stream\": False, \n", "           \"messages\": [\n", "               {\"role\": \"user\", \"content\": 'hello'}\n", "               ]\n", "           }"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["post_res = requests.post(api_url, headers=headers, json=request)\n", "print(post_res)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["llm_answer = eval(post_res.text)['data']['choices'][0]['message']['content']\n", "print(llm_answer)"], "outputs": [], "metadata": {}}, {"source": ["### 2.2 Your task: Use any local model to generate two long text \n", "\n", "You can choose any question, but each should let the LLM to generate over 300 words in english, while the other should generate 300 Chinese characters. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "# write a prompt, for example\n", "# prompt = '\u5e2e\u6211\u5199\u4e00\u7bc7\u6587\u7ae0\u6765\u4ecb\u7ecd\u5929\u5b89\u95e8\u7684\u80cc\u666f\u5386\u53f2\uff0c\u4ece\u53e4\u4ee3\u8bf4\u5230\u73b0\u4ee3\uff0c\u5305\u542b\u5f88\u591a\u8ddf\u5929\u5b89\u95e8\u6709\u5173\u7cfb\u7684\u6545\u4e8b\u3002\u8d8a\u957f\u8d8a\u597d\uff0c\u4e0d\u53ef\u4ee5\u5c11\u4e8e1000\u4e2a\u5b57\u3002'\n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "# prepare and call the service using a chinese prompt\n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "# prepare and call the service using a english prompt, for example, \n", "# prompt = 'Where is the capital of France?'\n"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####\n", "# prepare and call the service using a english prompt\n"], "outputs": [], "metadata": {}}, {"source": ["### 2.3 Your task (Homework): Write a simpler API for the local LLMs\n", "\n", "It is time to practise your Python skills.  Write an function similar to the OpenAI API and hide the raw headers and reponses from the callers. \n", "It should at least support setting models, sending prompts, and return results.  It should return error if the HTTP reply code is other than 200. \n", "\n", "Hint: you can try to use GitHub Copilot to help you write it."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["#### YOUR TASK ####"], "outputs": [], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "py310", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.10.13", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}
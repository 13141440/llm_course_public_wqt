{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 1: Try Cloud-based LLM API Services"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## You will learn:\n", "- First experience how to do run program on the cloud\n", "- Learn how to manage API keys\n", "- Frist experience of using different LLM APIs\n", "- (If you haven't used it before), how to use Jupyter Notebook in VSCode"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0 Preparations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 0.1 Dependencies"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["# requirements.txt contains the basic packages needed to implement this project\n", "# We have installed all dependencies in the default image, so you do not have to install them again, if you use the default image.\n", "#!pip install -r requirements.txt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 0.2 Saving your API token in a .env file"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["\n", "# Instead of hardcoding the OpenAI API key, use the dotenv package to load it securely from environment variables.\n", "# \n", "# Instructions to do it:\n", "# 1. Install the dotenv package if you haven't already by running: `pip install python-dotenv`\n", "# 2. Create a new file named .env in the root directory of your project. (AND Never commit it to Git!)\n", "# 3. The content in this file should be stored as key-value pair. The .env file is simply a text file with one key-value per line like:\n", "# \n", "#     # Comment 1\n", "#     KEY1=value1\n", "#     # Comment 2\n", "#     KEY2=value2\n", "# \n", "# 4. Load the environment variables in your Python code using the dotenv package:\n", "# \n", "#     from dotenv import load_dotenv\n", "#     import os\n", "#     load_dotenv()\n", "#     openai_api_key = os.environ.get(\"INFINI_API_KEY\")\n", "#     openai_base_url = os.environ.get(\"INFINI_BASE_URL\")\n", "# \n", "# More information see:\n", "# \n", "# https://pythonjishu.com/ifggzibrpkgavow/ "]}, {"cell_type": "markdown", "metadata": {}, "source": ["##  1 Using OpenAI API"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 Get response from a public API server"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# This code loads the OpenAI API key and base URL from environment variables using the dotenv package.\n", "# It ensures that sensitive information is not hardcoded in the script, enhancing security.\n", "\n", "from dotenv import load_dotenv\n", "import os\n", "load_dotenv()\n", "openai_api_key = os.environ.get(\"INFINI_API_KEY\")\n", "openai_base_url = os.environ.get(\"INFINI_BASE_URL\")\n", "\n", "print(openai_base_url)  "]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["from openai import OpenAI\n", "client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)\n", "\n", "# You can choose a model from the following list\n", "# Or you can log into your Infini-AI or SiliconFlow account, and find an available model you want to use.\n", "# model = \"Qwen/QVQ-72B-Preview\"\n", "# model=\"llama-3.3-70b-instruct\"\n", "model=\"deepseek-r1-distill-qwen-32b\"\n", "\n", "response = client.chat.completions.create(\n", "  model=model,\n", "  messages=[\n", "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n", "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n", "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n", "  ]\n", ")\n", "print(response)\n", "print(response.choices[0].message.content)"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["# pretty format the response\n", "import IPython\n", "IPython.display.Markdown(response.choices[0].message.content)"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["from openai import OpenAI\n", "client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)\n", "\n", "response = client.chat.completions.create(\n", "  model=model,\n", "  messages=[\n", "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n", "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n", "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n", "  ]\n", ")\n", "IPython.display.Markdown(response.choices[0].message.content)"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# You can exlore what information is in the response object by printing it out and examine it\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can learn more about the OpenAI API from https://platform.openai.com/docs/overview"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.2  Your Task: Try to find a question that Llama-3.3 cannot answer.\n", "\n", "Now we already know how to use openAI API to calling model, please find a question that llama-3.3-70b-instruct cannot answer or obvious need to improve."]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# Find the question\n"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# using the llama-3.3-70b model, create a chat response to the prompt above\n"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: can you make llama-3.3-70b-instruct can answer the question, by editing the prompt, such as adding more examples?  "]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: Repeat the query with a variation of qwen2.5-7b-instruct. Can it answer the question? If not, can you edit the prompt again to make it better, again?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.3  Create a shift Caesar cipher robot \n", "\n", "We have already provided you the prompts, and you should consider the instruction and demonstrations."]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["def encode(s):\n", "    for c in s:\n", "        if c not in ' ,.!?':\n", "            c = chr(ord(c) + 1)\n", "        print(c, end='')    \n", "        \n", "def decode(s):\n", "    for c in s:\n", "        if c not in ' ,.!?':\n", "            c = chr(ord(c) - 1)\n", "        print(c, end='')"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["encode('What is the capital of France?')"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["prompt = \"\"\"\n", "You are an expert on Caesar Cipher. We will communicate in Caesar. Do not be a translator.\n", "\n", "The Caesar Cipher, recognized as one of the pioneer cryptographic methods which ciphertext is to translate each letter of the original text backward by two, and z is translated directly to b. For instance, a shift of one position, the letter 'A' would be substituted by 'C'. you should answer my question in Caesar.\n", "\n", "Examples:\n", "\n", "User: ipx up nblf b cpnc ?\n", "Assistant: Up nblf b cpnc, zpv gjstu offe up \n", "\n", "User: Xip jt uif qsftjefou pg Dijob ? \n", "Assistant: Wh Ihmfohmf.\n", "\n", "User: Dbo zpv ufmm nf xifsf jt uif dbqjubm pg Dijob ?\n", "Assistant: Cfjkjoh.\n", "\n", "User: Dbo zpv ufmm nf xifsf jt uif dbqjubm pg Bnfsjdbo ?\n", "Assistant: Xbtijohupo.\n", "\n", "User: Xibu jt uif dbqjubm pg Gsbodf ?\"\"\""]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# using the llama-3.3-70b-instruct model, create a chat response to the prompt above\n"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: Print out the cipher text here\n", "### TODO: Print out the clear text here using the decode() function\n"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: print out the response object.  Explore the entire response object.  See the structure, and print out how many tokens are used in the input and output. \n"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: Repeat the query with another model 'qwen-2.5'.  Do you still get the same response?"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: (optional) can you let 'qwen-2.5' to print the same, by adding more examples in the prompt?  \n", "### Consider using a script to generate a much longer prompt with more examples"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Try another cloud-based API service: SiliconFlow"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "### TODO: Try another cloud-based API service, SiliconFlow. \n", "### Apply for a free API key from SiliconFlow.\n", "### Setup another .env file for SiliconFlow API key and base URL.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 Your task: Use a model of your choice on SiliconFlow to generate two long text \n", "\n", "You can choose any question, but each should let the LLM to generate over 300 words in english, while the other should generate 300 Chinese characters. "]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# write a prompt, for example\n", "# prompt = '\u5e2e\u6211\u5199\u4e00\u7bc7\u6587\u7ae0\u6765\u4ecb\u7ecd\u5929\u5b89\u95e8\u7684\u80cc\u666f\u5386\u53f2\uff0c\u4ece\u53e4\u4ee3\u8bf4\u5230\u73b0\u4ee3\uff0c\u5305\u542b\u5f88\u591a\u8ddf\u5929\u5b89\u95e8\u6709\u5173\u7cfb\u7684\u6545\u4e8b\u3002\u8d8a\u957f\u8d8a\u597d\uff0c\u4e0d\u53ef\u4ee5\u5c11\u4e8e1000\u4e2a\u5b57\u3002'\n"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# prepare and call the service using a chinese prompt\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 2}